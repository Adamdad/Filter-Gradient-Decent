import torchfrom torch.optim.optimizer import Optimizer, requiredimport pywtimport torch.nn.functional as Ffrom torch import nnclass WTGD(Optimizer):    r"""Implements stochastic gradient descent (optionally with momentum).    Nesterov momentum is based on the formula from    `On the importance of initialization and momentum in deep learning`__.    Args:        params (iterable): iterable of parameters to optimize or dicts defining            parameter groups        lr (float): learning rate        momentum (float, optional): momentum factor (default: 0)        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)        dampening (float, optional): dampening for momentum (default: 0)        nesterov (bool, optional): enables Nesterov momentum (default: False)    Example:        # >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)        # >>> optimizer.zero_grad()        # >>> loss_fn(model(input), target).backward()        # >>> optimizer.step()    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf    .. note::        The implementation of SGD with Momentum/Nesterov subtly differs from        Sutskever et. al. and implementations in some other frameworks.        Considering the specific case of Momentum, the update can be written as        .. math::                  v_{t+1} = \mu * v_{t} + g_{t+1} \\                  p_{t+1} = p_{t} - lr * v_{t+1}        where p, g, v and :math:`\mu` denote the parameters, gradient,        velocity, and momentum respectively.        This is in contrast to Sutskever et. al. and        other frameworks which employ an update of the form        .. math::             v_{t+1} = \mu * v_{t} + lr * g_{t+1} \\             p_{t+1} = p_{t} - v_{t+1}        The Nesterov version is analogously modified.    """    def __init__(self, params, lr=required, momentum=0, dampening=0, len_mem=8,                 wavelet='haar', thresh=0.2, enhance=5, level=1,                 weight_decay=0, nesterov=False, device=None):        self.thresh = thresh        self.level = level        self.enhance = enhance        self.wavelet = wavelet        self.len_mem = len_mem        if lr is not required and lr < 0.0:            raise ValueError("Invalid learning rate: {}".format(lr))        if momentum < 0.0:            raise ValueError("Invalid momentum value: {}".format(momentum))        if weight_decay < 0.0:            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))        self.device = device        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,                        weight_decay=weight_decay, nesterov=nesterov)        if nesterov and (momentum <= 0 or dampening != 0):            raise ValueError("Nesterov momentum requires a momentum and zero dampening")        super(WTGD, self).__init__(params, defaults)    def __setstate__(self, state):        super(WTGD, self).__setstate__(state)        for group in self.param_groups:            group.setdefault('nesterov', False)    def DWT(self, x):        shape = x.shape        x = x.view(self.len_mem, -1)        x = x.data.cpu().numpy().transpose()        coeffs = pywt.wavedec(x, self.wavelet, level=self.level)        # for i in range(self.level):        #     coeffs[i + 1] = pywt.threshold(coeffs[i + 1], self.thresh, mode='soft')        coeffs[0] *= self.enhance        denoise_x = pywt.waverec(coeffs, self.wavelet).transpose()        return torch.tensor(denoise_x).view(shape).to(self.device)    def step(self, closure=None):        """Performs a single optimization step.        Arguments:            closure (callable, optional): A closure that reevaluates the model                and returns the loss.        """        loss = None        if closure is not None:            loss = closure()        for group in self.param_groups:            weight_decay = group['weight_decay']            momentum = group['momentum']            dampening = group['dampening']            nesterov = group['nesterov']            for p in group['params']:                if p.grad is None:                    continue                d_p = p.grad.data                if weight_decay != 0:                    d_p.add_(weight_decay, p.data)                param_state = self.state[p]                if 'momentum_buffer' not in param_state:                    param_state['momentum_buffer'] = torch.stack(self.len_mem * [torch.clone(d_p).detach()])                    buf = param_state['momentum_buffer'][-1]                else:                    # push the moving average into the                    param_state['momentum_buffer'][:-1] = torch.clone(param_state['momentum_buffer'][1:])                    param_state['momentum_buffer'][-1] = torch.clone(d_p).detach()                    buf = self.DWT(param_state['momentum_buffer'])[-1]                if nesterov:                    d_p = d_p.add(momentum, buf)                else:                    d_p = buf                p.data.add_(-group['lr'], d_p)        return loss# class DWT1d_pytorch(nn.Module):#     def __init__(self, wave, pad, level, device):#         super(DWT1d_pytorch, self).__init__()#         if isinstance(wave, str):#             wave = pywt.Wavelet(wave)#         if isinstance(wave, pywt.Wavelet):#             h0, h1 = wave.dec_lo, wave.dec_hi#         self.h0 = torch.tensor(h0, device=device)#         self.h1 = torch.tensor(h1, device=device)#         self.pad = pad#         self.J = level#     def forward(self, x):#         ll = x#         yh = []#         for j in range(self.J):#             # Do 1 level of the transform#             ll, high = afb1d(ll, self.h0, self.h1, mode=self.pad)#             yh.append(high)#         return ll, yh### def mypad(x, pad, mode='constant', value=0):#     """ Function to do numpy like padding on tensors. Only works for 2-D#     padding.#     Inputs:#         x (tensor): tensor to pad#         pad (tuple): tuple of (left, right, top, bottom) pad sizes#         mode (str): 'symmetric', 'wrap', 'constant, 'reflect', 'replicate', or#             'zero'. The padding technique.#     """#     if mode == 'constant' or mode == 'reflect' or mode == 'replicate':#         return F.pad(x, pad, mode, value)#     elif mode == 'zero':#         return F.pad(x, pad)#     else:#         raise ValueError("Unkown pad type: {}".format(mode))## def afb1d(x, h0, h1, mode='zero', dim=1):#     """ 1D analysis filter bank (along one dimension only) of an image#     Inputs:#         x (tensor): 4D input with the last two dimensions the spatial input#         h0 (tensor): 4D input for the lowpass filter. Should have shape (1, 1,#             h, 1) or (1, 1, 1, w)#         h1 (tensor): 4D input for the highpass filter. Should have shape (1, 1,#             h, 1) or (1, 1, 1, w)#         mode (str): padding method#         dim (int) - dimension of filtering. d=2 is for a vertical filter (called#             column filtering but filters across the rows). d=3 is for a#             horizontal filter, (called row filtering but filters across the#             columns).#     Returns:#         lohi: lowpass and highpass subbands concatenated along the channel#             dimension#     """##     # Convert the dim to positive#     N = x.shape[-1]#     L = h0.numel()#     h = torch.cat([h0, h1], dim=0)##     # Calculate the pad size#     outsize = pywt.dwt_coeff_len(N, L, mode=mode)#     p = 2 * (outsize - 1) - N + L##     pad = (p//2, (p+1)//2)#     x = mypad(x, pad=pad, mode=mode)#     lohi = F.conv1d(x, h, stride=2)##     return lohi# if __name__ == '__main__':#     dwt = DWT1d_pytorch('haar', pad='reflect', device='cpu', level=1)#     img = torch.randn((128,128))#     result = dwt(img)#     print(result)