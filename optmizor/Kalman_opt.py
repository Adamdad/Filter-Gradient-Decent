import torchfrom torch.optim.optimizer import Optimizer, requiredclass KGD(Optimizer):    r"""Implements stochastic gradient descent (optionally with momentum).    Nesterov momentum is based on the formula from    `On the importance of initialization and momentum in deep learning`__.    Args:        params (iterable): iterable of parameters to optimize or dicts defining            parameter groups        lr (float): learning rate        momentum (float, optional): momentum factor (default: 0)        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)        dampening (float, optional): dampening for momentum (default: 0)        nesterov (bool, optional): enables Nesterov momentum (default: False)    Example:        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)        >>> optimizer.zero_grad()        >>> loss_fn(model(input), target).backward()        >>> optimizer.step()    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf    .. note::        The implementation of SGD with Momentum/Nesterov subtly differs from        Sutskever et. al. and implementations in some other frameworks.        Considering the specific case of Momentum, the update can be written as        .. math::                  v_{t+1} = \mu * v_{t} + g_{t+1} \\                  p_{t+1} = p_{t} - lr * v_{t+1}        where p, g, v and :math:`\mu` denote the parameters, gradient,        velocity, and momentum respectively.        This is in contrast to Sutskever et. al. and        other frameworks which employ an update of the form        .. math::             v_{t+1} = \mu * v_{t} + lr * g_{t+1} \\             p_{t+1} = p_{t} - v_{t+1}        The Nesterov version is analogously modified.    """    def __init__(self, params, lr=required, weight_decay=0, sigma_Q=0.01, sigma_R=2, gamma=5, momentum=0.9,                 device=None):        self.device = device        if lr is not required and lr < 0.0:            raise ValueError("Invalid learning rate: {}".format(lr))        if weight_decay < 0.0:            raise ValueError("Invalid weight_decay value: {}".format(weight_decay))        defaults = dict(lr=lr, sigma_Q=sigma_Q, sigma_R=sigma_R, gamma=gamma, momentum=momentum,                        weight_decay=weight_decay)        super(KGD, self).__init__(params, defaults)    def filter(self, y_t, x_t, P_t, sigma_Q, sigma_R, gamma):        state_dim = x_t.shape[0]        Q_t = sigma_Q * torch.eye(state_dim).to(self.device)        R_t = sigma_R * torch.eye(state_dim).to(self.device)        A_t = gamma * torch.eye(state_dim).to(self.device)        C_t = torch.eye(state_dim).to(self.device)        # Prediction Step        # x_{t+1}=A_t * x_t        x_t = torch.matmul(A_t, x_t)        P_t = torch.matmul(A_t, torch.matmul(P_t, A_t)) + Q_t        # Update Step        # compute measurement innovation, or the residual        r_t = y_t - torch.matmul(C_t, x_t)        # y_t should be a matrix        # can you combine these steps to speed up?        S_t = R_t + torch.matmul(C_t, torch.matmul(P_t, C_t))        K_t = torch.matmul(P_t, torch.matmul(C_t, torch.inverse(S_t)))        x_t = x_t + torch.matmul(K_t, r_t)        P_t = torch.matmul((torch.eye(state_dim, device=self.device) - torch.matmul(K_t, C_t)), P_t)        return x_t, P_t    def easy_filter(self, y_t, x_t, P_t, sigma_Q, sigma_R, gamma):        x_t = gamma * x_t        P_t = gamma ** 2 * P_t + sigma_Q        r_t = y_t - x_t        K_t = P_t / (sigma_R + P_t)        x_t = x_t + K_t * r_t        P_t = (1 - K_t) * P_t        return x_t, P_t    def step(self, closure=None):        """Performs a single optimization step.        Arguments:            closure (callable, optional): A closure that reevaluates the model                and returns the loss.        """        loss = None        if closure is not None:            loss = closure()        for group in self.param_groups:            weight_decay = group['weight_decay']            sigma_R = group['sigma_R']            sigma_Q = group['sigma_Q']            gamma = group['gamma']            for p in group['params']:                if p.grad is None:                    continue                d_p = p.grad.data                if weight_decay != 0:                    d_p.add_(weight_decay, p.data)                param_state = self.state[p]                if 'xt_buffer' not in param_state and 'Pt_buffer' not in param_state:                    buf_x = param_state['xt_buffer'] = torch.clone(d_p).detach()                    buf_P = param_state['Pt_buffer'] = 0.01                else:                    buf_x = param_state['xt_buffer']                    buf_P = param_state['Pt_buffer']                    buf_x, buf_P = self.easy_filter(d_p, buf_x, buf_P, sigma_Q, sigma_R, gamma)                    param_state['xt_buffer'] = buf_x                    param_state['Pt_buffer'] = buf_P                p.data.add_(-group['lr'], buf_x)        return loss